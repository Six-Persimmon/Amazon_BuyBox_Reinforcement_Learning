{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf5f0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from env.bertrand_env import BertrandPricingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59965843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q‐learning agent with three update modes:\n",
    "      - 'async': asynchronous (only update played action)\n",
    "      - 'sync_perfect': update all counterfactual profits exactly\n",
    "      - 'sync_downward': update using downward‐demand assumption\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_actions: int,\n",
    "                 alpha: float = 0.1,\n",
    "                 init_low: float = 10.0,\n",
    "                 init_high: float = 20.0,\n",
    "                 update_type: str = 'async',\n",
    "                 cost: float = 2.0,\n",
    "                 prices: np.ndarray = None):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        # initialize W(p) ~ U[init_low, init_high]\n",
    "        self.Q = np.random.uniform(init_low, init_high, size=n_actions)\n",
    "        self.update_type = update_type\n",
    "        self.cost = cost\n",
    "        self.prices = prices  # needed for synchronous updates\n",
    "\n",
    "    def select_action(self) -> int:\n",
    "        # greedy policy (no explicit exploration beyond init)\n",
    "        return int(np.argmax(self.Q))\n",
    "\n",
    "    def update(self,\n",
    "               action: int,\n",
    "               reward: float,\n",
    "               competitor_action: int):\n",
    "        p_j = self.prices[competitor_action]\n",
    "        p_i = self.prices[action]\n",
    "        # realized quantity at chosen price\n",
    "        if p_i < p_j and p_i <= 10:\n",
    "            q = 1.0\n",
    "        elif p_i == p_j and p_i <= 10:\n",
    "            q = 0.5\n",
    "        else:\n",
    "            q = 0.0\n",
    "        base_profit = (p_i - self.cost) * q\n",
    "\n",
    "        if self.update_type == 'async':\n",
    "            # only update played price\n",
    "            self.Q[action] = self.alpha * reward + (1 - self.alpha) * self.Q[action]\n",
    "\n",
    "        elif self.update_type == 'sync_perfect':\n",
    "            # update for all p exactly\n",
    "            for i, p in enumerate(self.prices):\n",
    "                # compute counterfactual demand\n",
    "                if p < p_j and p <= 10:\n",
    "                    d = 1.0\n",
    "                elif p == p_j and p <= 10:\n",
    "                    d = 0.5\n",
    "                else:\n",
    "                    d = 0.0\n",
    "                profit = (p - self.cost) * d\n",
    "                self.Q[i] = self.alpha * profit + (1 - self.alpha) * self.Q[i]\n",
    "\n",
    "        elif self.update_type == 'sync_downward':\n",
    "            # first update the chosen action as in async\n",
    "            self.Q[action] = self.alpha * reward + (1 - self.alpha) * self.Q[action]\n",
    "            # then adjust others under downward demand assumption\n",
    "            for i, p in enumerate(self.prices):\n",
    "                if i == action:\n",
    "                    continue\n",
    "                # case p > chosen: W(p) can only fall to base_profit\n",
    "                if p > p_i and self.Q[i] > base_profit:\n",
    "                    pi_e = base_profit\n",
    "                    self.Q[i] = self.alpha * pi_e + (1 - self.alpha) * self.Q[i]\n",
    "                # case p < chosen: if (p-c)*q exceeds current W(p), update upward\n",
    "                elif p < p_i and (p - self.cost) * q > self.Q[i]:\n",
    "                    pi_e = (p - self.cost) * q\n",
    "                    self.Q[i] = self.alpha * pi_e + (1 - self.alpha) * self.Q[i]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown update_type: {self.update_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(env, agent1, agent2, periods: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run a single trajectory of length `periods`, return sequence of prices chosen by firm 1.\n",
    "    \"\"\"\n",
    "    history = np.zeros(periods)\n",
    "    for t in range(periods):\n",
    "        a1 = agent1.select_action()\n",
    "        a2 = agent2.select_action()\n",
    "        _, (r1, r2), _, _ = env.step((a1, a2))\n",
    "        agent1.update(a1, r1, a2)\n",
    "        agent2.update(a2, r2, a1)\n",
    "        history[t] = env.prices[a1]\n",
    "    return history\n",
    "\n",
    "\n",
    "def run_all(protocol: str,\n",
    "            Sims: int = 100,\n",
    "            periods: int = 5000,\n",
    "            alpha: float = 0.1):\n",
    "    # environment & common params\n",
    "    grid_size = 100\n",
    "    price_min, price_max = 0.1, 10.0\n",
    "    cost = 2.0\n",
    "    prices = np.linspace(price_min, price_max, grid_size)\n",
    "    env = BertrandPricingEnv(price_min, price_max, grid_size, cost)\n",
    "\n",
    "    all_hist = np.zeros((Sims, periods))\n",
    "    for sim in range(Sims):\n",
    "        # new agents each sim\n",
    "        ag1 = QLearningAgent(grid_size, alpha, 10, 20, protocol, cost, prices)\n",
    "        ag2 = QLearningAgent(grid_size, alpha, 10, 20, protocol, cost, prices)\n",
    "        hist = simulate(env, ag1, ag2, periods)\n",
    "        all_hist[sim] = hist\n",
    "\n",
    "    # compute percentiles by period\n",
    "    q_min, q25, q50, q75, q_max = np.percentile(all_hist, [0,25,50,75,100], axis=0)\n",
    "    return q_min, q25, q50, q75, q_max\n",
    "\n",
    "\n",
    "def plot_results():\n",
    "    protocols = ['async', 'sync_perfect', 'sync_downward']\n",
    "    periods = {'async':5000, 'sync_perfect':5000, 'sync_downward':5000}\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for prot in protocols:\n",
    "        print(f\"Running protocol: {prot}\")  \n",
    "        qmin, q25, q50, q75, qmax = run_all(prot, periods=periods[prot])\n",
    "        xs = np.arange(len(q50))\n",
    "        plt.plot(xs, q50, label=f\"Median ({prot})\")\n",
    "    plt.xlabel('Period')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Price Convergence under Different Update Protocols')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the simulation and plot results\n",
    "plot_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
